================================================================================
        BÁO CÁO KỸ THUẬT HỆ THỐNG TÌM KIẾM TIN TỨC QUÂN SỰ VIỆT NAM
================================================================================

Tên dự án: Vietnamese Military News Crawler & Search Engine
Ngày báo cáo: 11/01/2026
Phiên bản: 2.0

================================================================================
CHƯƠNG 1: TỔNG QUAN
================================================================================

1.1. Mục tiêu đề tài
--------------------

Xây dựng hệ thống thu thập tự động và tìm kiếm tin tức quân sự từ các nguồn
báo điện tử Việt Nam. Hệ thống cho phép người dùng tìm kiếm nhanh chóng các
bài báo liên quan thông qua từ khóa tiếng Việt với độ chính xác cao, đồng
thời giải thích lý do xếp hạng của mỗi kết quả.

Các mục tiêu cụ thể:
- Thu thập song song từ 4 nguồn: VNExpress, Dân Trí, VietnamNet, QDND
- Đánh chỉ mục tự động vào Elasticsearch với hỗ trợ tiếng Việt
- Tìm kiếm toàn văn với xếp hạng thông minh dựa trên BM25
- Xử lý tìm kiếm có dấu và không dấu với mức độ ưu tiên khác nhau
- Hiển thị kết quả với điểm số và trích đoạn giải thích

1.2. Kiến trúc tổng quan
------------------------

Quy trình xử lý của hệ thống gồm 5 giai đoạn:

    Thu thập dữ liệu → Đánh chỉ mục → Xử lý truy vấn → Xếp hạng → Trả về kết quả
    (Crawling)        (Indexing)     (Query Processing) (Ranking)  (Results)

Sơ đồ luồng dữ liệu:

    ┌─────────────────────────────────────────────────────────────────────────┐
    │                           NGUỒN DỮ LIỆU                                 │
    │     VNExpress ──┬── Dân Trí ──┬── VietnamNet ──┬── QDND                 │
    └─────────────────┼─────────────┼────────────────┼────────────────────────┘
                      v             v                v
    ┌─────────────────────────────────────────────────────────────────────────┐
    │                        THU THẬP DỮ LIỆU                                 │
    │  ┌────────────────────────────────────────────────────────────────┐     │
    │  │ UnifiedCrawler                                                 │     │
    │  │ - Phát hiện URL bài báo từ trang danh sách                     │     │
    │  │ - Trích xuất nội dung: tiêu đề, ngày, nội dung                 │     │
    │  └────────────────────────────────────────────────────────────────┘     │
    └─────────────────────────────────┬───────────────────────────────────────┘
                                      v
    ┌─────────────────────────────────────────────────────────────────────────┐
    │                        ĐÁNH CHỈ MỤC                                     │
    │  ┌────────────────────────────────────────────────────────────────┐     │
    │  │ ElasticIndexer                                                 │     │
    │  │ - Parse nội dung, tạo Document ID (MD5 của title + source)     │     │
    │  │ - Phân tích từ: ICU Tokenizer + Stopwords tiếng Việt           │     │
    │  │ - Lưu trữ 2 phiên bản: có dấu và không dấu                     │     │
    │  └────────────────────────────────────────────────────────────────┘     │
    └─────────────────────────────────┬───────────────────────────────────────┘
                                      v
    ┌─────────────────────────────────────────────────────────────────────────┐
    │                     ELASTICSEARCH INDEX                                 │
    │  ┌────────────────────────────────────────────────────────────────┐     │
    │  │ Index: news_quansu                                             │     │
    │  │ - Inverted Index cho title, body (có dấu)                      │     │
    │  │ - Inverted Index cho title.no_accent, body.no_accent           │     │
    │  │ - Metadata: source, category, publish_date, url                │     │
    │  └────────────────────────────────────────────────────────────────┘     │
    └─────────────────────────────────┬───────────────────────────────────────┘
                                      v
    ┌─────────────────────────────────────────────────────────────────────────┐
    │                        TÌM KIẾM VÀ XẾP HẠNG                             │
    │  ┌────────────────────────────────────────────────────────────────┐     │
    │  │ Search Engine                                                  │     │
    │  │ - Multi-match query với 4 mức độ ưu tiên                       │     │
    │  │ - BM25 scoring với field boosting                              │     │
    │  │ - Highlight từ khóa trong kết quả                              │     │
    │  └────────────────────────────────────────────────────────────────┘     │
    └─────────────────────────────────────────────────────────────────────────┘

================================================================================
CHƯƠNG 2: THU THẬP VÀ XỬ LÝ DỮ LIỆU
================================================================================

2.1. Nguồn dữ liệu và phạm vi
-----------------------------

Hệ thống thu thập tin tức quân sự từ 4 nguồn báo điện tử lớn tại Việt Nam:

    ┌────────────┬────────────────────────────────────┬──────────────────────┐
    │ Nguồn      │ URL gốc                            │ Chuyên mục           │
    ├────────────┼────────────────────────────────────┼──────────────────────┤
    │ VNExpress  │ https://vnexpress.net              │ the-gioi/quan-su     │
    │ Dân Trí    │ https://dantri.com.vn              │ the-gioi/quan-su     │
    │ VietnamNet │ https://vietnamnet.vn              │ the-gioi/quan-su     │
    │ QDND       │ https://www.qdnd.vn                │ quoc-te/quan-su-the-gioi │
    └────────────┴────────────────────────────────────┴──────────────────────┘

Phạm vi thu thập:
- Số trang danh sách: 10 trang mỗi nguồn (cấu hình được)
- Tần suất cập nhật: 30 phút/lần (chế độ liên tục)
- Ước tính: 200-300 URL mới mỗi lần quét

2.2. Chi tiết kỹ thuật trích xuất theo nguồn tin
-------------------------------------------------

Hệ thống sử dụng phương pháp xử lý đồng nhất cho tất cả các nguồn tin:

**Phương pháp chung:**
- Sử dụng `requests.get()` với timeout 20s
- Parse bằng `BeautifulSoup(response.content, "html.parser")`
- Trích xuất theo cấu trúc HTML đặc thù của từng site
- Xử lý encoding tự động bởi BeautifulSoup

**Cấu trúc HTML của từng nguồn:**

    ┌─────────────┬─────────────────────┬─────────────────────┬─────────────────────┐
    │ Nguồn       │ Tiêu đề             │ Ngày tháng          │ Nội dung            │
    ├─────────────┼─────────────────────┼─────────────────────┼─────────────────────┤
    │ VNExpress   │ h1.title-detail     │ span.date           │ p.Normal            │
    │ Dân Trí     │ h1.title-page       │ time.author-time    │ div.singular-content│
    │ VietnamNet  │ h1.content-detail-  │ div.bread-crumb-    │ div.main-content    │
    │             │ title               │ detail__time        │                     │
    │ QDND        │ h1 / og:title       │ JSON-LD data        │ div.detail-content  │
    └─────────────┴─────────────────────┴─────────────────────┴─────────────────────┘

**Đặc điểm riêng biệt của VNExpress:**

VNExpress là nguồn duy nhất có cơ chế chống bot nghiêm ngặt:

- **Cơ chế chặn**: Chặn theo dải IP/subnet thay vì IP đơn lẻ
- **Thời gian chặn**: Kéo dài 30+ phút, chỉ bypass được bằng VPN
- **Phát hiện bot**: Dựa trên pattern và tần suất request

**Xử lý đặc biệt cho VNExpress:**
```
Thuật toán phát hiện block:
1. Đếm consecutive_timeouts cho mỗi request timeout
2. Nếu consecutive_timeouts ≥ 3: đánh dấu is_blocked = True
3. Khi is_blocked = True: bỏ qua tất cả request tiếp theo
4. Reset trạng thái ở chu kỳ crawl tiếp theo
```

**Lợi ích của cách xử lý:**
- Phát hiện block nhanh chóng (sau 3 timeout)
- Không làm chậm 3 crawler còn lại
- Tự động thử lại ở chu kỳ tiếp theo
- 75% dữ liệu vẫn được thu thập bình thường

**Ba nguồn còn lại (Dân Trí, VietnamNet, QDND):**
- Hoạt động ổn định, không có cơ chế chống bot
- Xử lý đồng nhất với timeout tiêu chuẩn
- Ít khi gặp lỗi, tỷ lệ thành công cao

2.3. Giải thuật phát hiện liên kết
----------------------------------

Thuật toán phát hiện URL bài báo từ trang danh sách:

    INPUT:  article_type (chuyên mục), total_pages (số trang)
    OUTPUT: unique_urls (tập hợp URL không trùng lặp)

    1. Khởi tạo unique_urls = ∅
    2. FOR page = 1 TO total_pages DO
    3.     page_url ← construct_url(article_type, page)
    4.     html_content ← HTTP_GET(page_url, timeout=30s)
    5.     article_links ← parse_links(html_content, pattern)
    6.     unique_urls ← unique_urls ∪ article_links
    7. END FOR
    8. RETURN unique_urls

Trong đó:
- construct_url(): Tạo URL trang danh sách theo định dạng của từng nguồn
- parse_links(): Sử dụng BeautifulSoup4 để trích xuất thẻ <a> trong các
  class đặc trưng của từng nguồn

2.4. Kiến trúc Crawler
----------------------

Hệ thống sử dụng mô hình Factory Pattern để quản lý các crawler:

    BaseCrawler (Abstract Base Class)
         │
         ├─── VNExpressCrawler (với cơ chế phát hiện block)
         ├─── DanTriCrawler
         ├─── VietnamNetCrawler
         └─── QDNDCrawler

    UnifiedCrawler
         │
         └─── Điều phối chạy song song các crawler con

Các phương thức chính của BaseCrawler:
- get_urls_of_type_thread(): Lấy danh sách URL từ trang danh sách
- extract_content(): Trích xuất nội dung từ URL bài báo
- write_content(): Ghi nội dung ra file
- crawl_once(): Thực hiện một chu kỳ crawl

2.5. Quy trình xử lý và làm sạch dữ liệu
-----------------------------------------

Thuật toán xử lý nội dung bài báo:

    INPUT:  raw_content (nội dung thô), source, category, url
    OUTPUT: structured_document (tài liệu có cấu trúc)

    1. lines ← SPLIT(raw_content, '\n')
    2. IF lines = ∅ THEN RETURN NULL
    3.
    4. title ← TRIM(lines[0])
    5.
    6. // Trích xuất ngày tháng
    7. IF LENGTH(lines) > 1 AND "Ngày:" ∈ lines[1] THEN
    8.     date_str ← EXTRACT_DATE(lines[1])
    9.     publish_date ← PARSE_DATE(date_str, "dd/MM/yyyy")
    10.    body_start ← 2
    11. ELSE
    12.    body_start ← 1
    13. END IF
    14.
    15. body ← JOIN(lines[body_start:], '\n')
    16.
    17. // Tạo định danh duy nhất để tránh trùng lặp
    18. unique_key ← CONCAT(title, "_", source)
    19. doc_id ← MD5(unique_key)
    20.
    21. RETURN {
    22.    "_id": doc_id,
    23.    "title": title,
    24.    "publish_date": publish_date,
    25.    "body": body,
    26.    "source": source,
    27.    "category": category,
    28.    "url": url
    29. }

Chiến lược tránh trùng lặp (Deduplication):
- Sử dụng title + source làm khóa định danh
- Áp dụng hàm băm MD5 để tạo Document ID
- Elasticsearch tự động update nếu ID đã tồn tại

2.6. Cơ chế cập nhật dữ liệu
----------------------------

Hệ thống hỗ trợ hai chế độ hoạt động:

A. Chế độ chạy một lần (Single Mode):
   - Crawl tất cả các nguồn một lần
   - Phù hợp cho thu thập dữ liệu ban đầu

B. Chế độ liên tục (Continuous Mode):
   - Lặp lại quá trình crawl theo chu kỳ
   - Khoảng cách mặc định: 1800 giây (30 phút)
   - Tự động reset trạng thái block của VNExpress mỗi chu kỳ

Thuật toán chế độ liên tục:

    INPUT:  crawlers[], crawl_interval
    OUTPUT: Cập nhật liên tục database

    1. cycle ← 1
    2. WHILE TRUE DO
    3.     // Reset trạng thái block cho VNExpress
    4.     FOR EACH crawler IN crawlers DO
    5.         IF crawler HAS reset_blocked_status THEN
    6.             crawler.reset_blocked_status()
    7.         END IF
    8.     END FOR
    9.
    10.    // Chạy song song các crawler
    11.    PARALLEL FOR EACH crawler IN crawlers DO
    12.        crawler.crawl_once()
    13.    END PARALLEL FOR
    14.
    15.    show_statistics()
    16.    SLEEP(crawl_interval)
    17.    cycle ← cycle + 1
    18. END WHILE

Cơ chế phát hiện block của VNExpress:

VNExpress triển khai hệ thống chống bot tinh vi với các đặc điểm:

A. Cơ chế chặn:
   - Chặn theo dải IP/subnet thay vì IP đơn lẻ
   - Thời gian chặn kéo dài (30+ phút)
   - Chỉ có thể bypass bằng VPN hoặc đổi mạng hoàn toàn
   - Không phân biệt thiết bị, chặn toàn bộ mạng

B. Thuật toán phát hiện trong hệ thống:
   - Đếm số lần timeout liên tục (consecutive_timeouts)
   - Nếu consecutive_timeouts ≥ 3: đánh dấu is_blocked = True
   - Khi is_blocked = True: bỏ qua tất cả request, không làm chậm các crawler khác
   - Reset về False ở chu kỳ tiếp theo

C. Lợi ích của cách xử lý hiện tại:
   - Phát hiện block nhanh chóng (sau 3 request)
   - Không làm chậm toàn hệ thống
   - 3/4 nguồn dữ liệu vẫn hoạt động bình thường
   - Tự động thử lại ở chu kỳ tiếp theo

2.7. Chiến lược lưu trữ
-----------------------

Dữ liệu được lưu trữ ở hai nơi:

A. File hệ thống (Backup):
   - Đường dẫn: result/{source}_quansu/{category}/url_{id}.txt
   - Định dạng: Plain text với tiêu đề, ngày, nội dung

B. Elasticsearch (Primary):
   - Index: news_quansu
   - Hỗ trợ tìm kiếm toàn văn
   - Inverted index tự động

================================================================================
CHƯƠNG 3: ĐÁNH CHỈ MỤC VÀ CẤU HÌNH ELASTICSEARCH
================================================================================

3.1. Thiết kế Schema
--------------------

Index news_quansu được thiết kế với các trường sau:

    ┌──────────────────┬───────────┬────────────────────────────────────────┐
    │ Trường           │ Kiểu      │ Mô tả                                  │
    ├──────────────────┼───────────┼────────────────────────────────────────┤
    │ title            │ text      │ Tiêu đề bài báo (có dấu)               │
    │ title.no_accent  │ text      │ Tiêu đề không dấu (sub-field)          │
    │ body             │ text      │ Nội dung bài báo (có dấu)              │
    │ body.no_accent   │ text      │ Nội dung không dấu (sub-field)         │
    │ publish_date     │ date      │ Ngày đăng (yyyy-MM-dd)                 │
    │ publish_date_str │ text      │ Ngày đăng dạng chuỗi gốc               │
    │ source           │ keyword   │ Nguồn tin (cho aggregation)            │
    │ category         │ keyword   │ Chuyên mục                             │
    │ url              │ keyword   │ Đường dẫn gốc                          │
    └──────────────────┴───────────┴────────────────────────────────────────┘

Cấu hình index:
- number_of_shards: 1 (phù hợp dataset nhỏ-trung bình)
- number_of_replicas: 0 (single node deployment)

Giải thích các thuật ngữ Elasticsearch quan trọng:

A. Field Types:
   - text: Được phân tích (analyzed) để tìm kiếm full-text
   - keyword: Không phân tích, dùng cho exact match, aggregation, sorting
   - date: Kiểu ngày tháng với format cụ thể

B. Multi-fields:
   - title.no_accent: Sub-field của title với analyzer khác
   - Cho phép cùng một dữ liệu được index theo nhiều cách
   - Ví dụ: title (có dấu) và title.no_accent (không dấu)

C. Index Settings:
   - Shards: Phân chia dữ liệu để tăng tốc độ tìm kiếm và song song hóa
   - Replicas: Sao chép dữ liệu để đảm bảo high availability
   - Analysis: Cấu hình cách xử lý và phân tích văn bản

3.2. Cấu hình Phân tích từ (Text Analysis)
------------------------------------------

Hệ thống sử dụng analyzer chuẩn của Elasticsearch để xử lý tiếng Việt:

A. Vietnamese Analyzer (có dấu):
   - Tokenizer: standard
   - Filters: lowercase → vietnamese_stop

B. Vietnamese No Accent Analyzer (không dấu):
   - Tokenizer: standard
   - Filters: lowercase → ascii_folding → vietnamese_stop

Giải thích các thành phần Text Analysis:

A. Tokenizer:
   - Nhiệm vụ: Tách văn bản thành các tokens (từ đơn)
   - standard: Tokenizer chuẩn của Elasticsearch, tách theo khoảng trắng và dấu câu
   - Xử lý Unicode cơ bản, phù hợp cho tiếng Việt

B. Token Filters:
   - lowercase: Chuyển tất cả về chữ thường để tìm kiếm case-insensitive
   - ascii_folding: Chuyển đổi ký tự có dấu thành không dấu (á→a, đ→d)
   - vietnamese_stop: Loại bỏ stopwords tiếng Việt

C. Analysis Chain:
   Text → standard tokenizer → lowercase → ascii_folding/stop → Final Tokens

Danh sách Stopwords tiếng Việt (80+ từ):
- Đại từ: tôi, tao, mình, ta, chúng tôi, chúng ta, họ, nó, ông, bà, anh, chị, em...
- Chức năng ngữ pháp: bị, bởi, cho, của, từ, với, theo, tại, qua, ra...
- Định lượng và chỉ định: cả, các, cái, mỗi, một cách, này, đó, những...
- Liên từ và trạng từ: và, nhưng, như, nên, nếu, vì, mà, rất, rồi...
- Động từ phụ trợ: có, có thể, được, đã, đang, sẽ, vẫn, phải...

Quy trình tokenize:

    Input:  "Quân đội Việt Nam tập trận ở Biển Đông"

    Vietnamese Analyzer (standard tokenizer):
    → ["quân", "đội", "việt", "nam", "tập", "trận", "biển", "đông"]
    (Loại bỏ "ở" vì là stopword)

    Vietnamese No Accent:
    → ["quan", "doi", "viet", "nam", "tap", "tran", "bien", "dong"]

3.3. Quy trình Indexing
-----------------------

Thuật toán index một bài báo:

    INPUT:  article (tài liệu đã parse)
    OUTPUT: success/failure

    1. doc_id ← article["_id"]
    2. doc_body ← {k: v | (k, v) ∈ article AND k ≠ "_id"}
    3.
    4. TRY
    5.     elasticsearch.index(
    6.         index = "news_quansu",
    7.         id = doc_id,
    8.         document = doc_body
    9.     )
    10.    RETURN TRUE
    11. CATCH Exception
    12.    RETURN FALSE

Đặc điểm:
- Sử dụng doc_id để thực hiện upsert (insert hoặc update)
- Nếu doc_id đã tồn tại: cập nhật document
- Nếu doc_id chưa có: thêm mới document

Bulk Indexing:
- Sử dụng Elasticsearch Bulk API để tăng tốc
- Giảm số lượng HTTP round-trips
- Throughput tăng 10-50 lần so với single insert

Cấu trúc Inverted Index được Elasticsearch tự động xây dựng:

Inverted Index là cấu trúc dữ liệu cốt lõi cho tìm kiếm full-text:

    Term          → Document IDs + Metadata
    ─────────────────────────────────────────
    quân          → [doc1, doc3, doc7] (TF: [3,1,2], Positions: [...])
    đội           → [doc1, doc3] (TF: [2,1], Positions: [...])
    tên           → [doc2, doc5, doc8] (TF: [1,2,1], Positions: [...])
    lửa           → [doc2, doc5, doc8] (TF: [3,1,4], Positions: [...])
    iskander      → [doc2, doc9] (TF: [2,1], Positions: [...])
    ...

Giải thích các thành phần Inverted Index:

A. Term Dictionary:
   - Danh sách tất cả các terms (từ) xuất hiện trong corpus
   - Được sắp xếp theo thứ tự alphabet để tìm kiếm nhanh
   - Mỗi term liên kết với posting list

B. Posting List:
   - Danh sách documents chứa term đó
   - Term Frequency (TF): Số lần term xuất hiện trong mỗi document
   - Positions: Vị trí của term trong document (cho phrase query)

C. Document Frequency (DF):
   - Số documents chứa term đó
   - Dùng để tính IDF (Inverse Document Frequency)

Quy trình tìm kiếm với Inverted Index:

Query: "tên lửa iskander"
1. Tokenize: ["tên", "lửa", "iskander"]
2. Lookup từng term trong Term Dictionary
3. Lấy posting lists:
   - "tên" → [doc2, doc5, doc8]
   - "lửa" → [doc2, doc5, doc8]
   - "iskander" → [doc2, doc9]
4. Intersect/Union các lists theo query type
5. Tính BM25 score cho từng document

================================================================================
CHƯƠNG 4: CƠ CHẾ TÌM KIẾM VÀ XẾP HẠNG
================================================================================

4.1. Xây dựng truy vấn (Query Construction)
-------------------------------------------

Hệ thống sử dụng Bool Query với 4 mức độ ưu tiên:

    ┌─────────────────────────────────────────────────────────────────────────┐
    │                           BOOL QUERY                                    │
    │  ┌───────────────────────────────────────────────────────────────────┐  │
    │  │ MUST (bắt buộc ít nhất 1 trong các SHOULD match)                  │  │
    │  │  ┌─────────────────────────────────────────────────────────────┐  │  │
    │  │  │ SHOULD clauses (minimum_should_match = 1):                  │  │  │
    │  │  │                                                             │  │  │
    │  │  │ 1. Match có dấu chính xác (boost = 10)                      │  │  │
    │  │  │    fields: title^5, body                                    │  │  │
    │  │  │                                                             │  │  │
    │  │  │ 2. Phrase match có dấu (boost = 15)                         │  │  │
    │  │  │    fields: title^10, body^2, slop = 2                       │  │  │
    │  │  │                                                             │  │  │
    │  │  │ 3. Match không dấu (boost = 7.5)                            │  │  │
    │  │  │    fields: title.no_accent^5, body.no_accent                │  │  │
    │  │  │                                                             │  │  │
    │  │  │ 4. Fuzzy match (boost = 2)                                  │  │  │
    │  │  │    fields: title^5, body, fuzziness = AUTO                  │  │  │
    │  │  └─────────────────────────────────────────────────────────────┘  │  │
    │  └───────────────────────────────────────────────────────────────────┘  │
    └─────────────────────────────────────────────────────────────────────────┘

Giải thích các mức độ ưu tiên:

    ┌──────┬────────────────────────────┬───────┬────────────────────────────┐
    │ STT  │ Loại match                 │ Boost │ Mục đích                   │
    ├──────┼────────────────────────────┼───────┼────────────────────────────┤
    │ 1    │ Phrase match có dấu        │ 15    │ Ưu tiên cao nhất cho cụm   │
    │      │                            │       │ từ khớp chính xác          │
    ├──────┼────────────────────────────┼───────┼────────────────────────────┤
    │ 2    │ Match có dấu               │ 10    │ Từ có dấu khớp chính xác   │
    ├──────┼────────────────────────────┼───────┼────────────────────────────┤
    │ 3    │ Match không dấu            │ 7.5   │ Fallback khi người dùng    │
    │      │                            │       │ không gõ dấu               │
    ├──────┼────────────────────────────┼───────┼────────────────────────────┤
    │ 4    │ Fuzzy match                │ 2     │ Xử lý lỗi chính tả         │
    └──────┴────────────────────────────┴───────┴────────────────────────────┘

Field Boosting:
- title^5: Match trong tiêu đề được nhân 5 lần điểm
- title^10: Phrase match trong tiêu đề được nhân 10 lần
- body^2: Phrase match trong nội dung được nhân 2 lần

4.2. Thuật toán Xếp hạng
------------------------

Elasticsearch sử dụng thuật toán BM25 (Best Matching 25) làm cơ sở tính điểm.

Công thức BM25:

    Score(D,Q) = Σ IDF(qᵢ) × [f(qᵢ,D) × (k₁ + 1)] / [f(qᵢ,D) + k₁ × (1 - b + b × |D|/avgdl)]

Trong đó:
- D: Document (tài liệu/bài báo)
- Q: Query (truy vấn)
- qᵢ: Term thứ i trong query
- f(qᵢ,D): Tần suất term qᵢ xuất hiện trong D (Term Frequency - TF)
- |D|: Độ dài document D (số từ)
- avgdl: Độ dài trung bình của tất cả documents
- k₁: Tham số điều chỉnh TF saturation (mặc định: 1.2)
- b: Tham số điều chỉnh ảnh hưởng độ dài document (mặc định: 0.75)

Công thức IDF (Inverse Document Frequency):

    IDF(qᵢ) = ln(1 + (N - n(qᵢ) + 0.5) / (n(qᵢ) + 0.5))

Trong đó:
- N: Tổng số documents trong index
- n(qᵢ): Số documents chứa term qᵢ

Ý nghĩa các thành phần BM25:

A. Term Frequency (TF):
   - Từ xuất hiện nhiều lần → điểm cao hơn
   - Nhưng có giới hạn (saturation) để tránh spam từ khóa
   - Saturation: TF càng tăng, điểm tăng chậm dần (logarithmic)

B. Inverse Document Frequency (IDF):
   - Từ hiếm (ít documents chứa) → điểm cao hơn
   - Từ phổ biến (nhiều documents chứa) → điểm thấp hơn
   - Ví dụ: "iskander" hiếm → IDF cao, "và" phổ biến → IDF thấp

C. Document Length Normalization:
   - Bài ngắn có từ khớp → điểm cao hơn bài dài
   - Tham số b điều chỉnh mức độ ảnh hưởng
   - b = 0: không chuẩn hóa, b = 1: chuẩn hóa hoàn toàn

D. Saturation Parameter (k₁):
   - Điều chỉnh mức độ saturation của TF
   - k₁ càng cao → TF ảnh hưởng mạnh hơn
   - k₁ = 0: TF không ảnh hưởng, chỉ có IDF

Điểm cuối cùng của document:

    Final_Score = BM25_Score × Field_Boost × Query_Boost

Xử lý lỗi chính tả (Fuzziness):
- Sử dụng khoảng cách Levenshtein (edit distance)
- Fuzziness = "AUTO":
  + Từ 1-2 ký tự: không cho phép sai
  + Từ 3-5 ký tự: cho phép sai 1 ký tự
  + Từ >5 ký tự: cho phép sai 2 ký tự

Phrase Matching với Slop:
- slop = 2: Cho phép tối đa 2 từ xen giữa các terms
- Điểm giảm dần theo slop (slop=0 cao nhất, slop=2 thấp hơn)

Chiến lược sắp xếp kết quả:
1. Primary: _score (giảm dần) - Bài điểm cao hơn lên trước
2. Secondary: publish_date (giảm dần) - Nếu cùng điểm, bài mới hơn lên trước

4.3. Cơ chế Hiển thị và Trích đoạn (Highlighting)
-------------------------------------------------

Mục đích: Giải thích cho người dùng tại sao bài báo được chọn và xếp hạng.

Cấu hình highlighting:

    ┌─────────────────────────────────────────────────────────────────────────┐
    │ Title Highlighting:                                                     │
    │ - pre_tags: ["<mark>"]                                                  │
    │ - post_tags: ["</mark>"]                                                │
    │ - number_of_fragments: 0 (hiển thị toàn bộ tiêu đề)                    │
    ├─────────────────────────────────────────────────────────────────────────┤
    │ Body Highlighting:                                                      │
    │ - pre_tags: ["<mark>"]                                                  │
    │ - post_tags: ["</mark>"]                                                │
    │ - fragment_size: 150 (ký tự mỗi đoạn)                                  │
    │ - number_of_fragments: 3 (top 3 đoạn tốt nhất)                         │
    └─────────────────────────────────────────────────────────────────────────┘

Quy trình highlighting:
1. Elasticsearch tìm các đoạn văn bản chứa query terms
2. Đánh dấu terms bằng thẻ <mark>
3. Trích xuất 3 đoạn tốt nhất từ body (mỗi đoạn tối đa 150 ký tự)
4. Trích xuất toàn bộ tiêu đề nếu có match

Kết quả trả về cho mỗi document:
- score: Điểm xếp hạng
- matched_in_title: Tiêu đề với từ khóa được đánh dấu
- matched_in_body: Các đoạn trích với từ khóa được đánh dấu

================================================================================
CHƯƠNG 5: KẾT LUẬN
================================================================================

Hệ thống Vietnamese Military News Crawler & Search Engine đã triển khai thành
công một pipeline hoàn chỉnh từ thu thập dữ liệu đến tìm kiếm và xếp hạng.

Các thành tựu đạt được:

1. Thu thập dữ liệu:
   - Crawl song song từ 4 nguồn báo lớn
   - Xử lý anti-bot của VNExpress
   - Tránh trùng lặp hiệu quả với MD5 hash

2. Xử lý tiếng Việt:
   - Standard tokenizer của Elasticsearch
   - Stopwords tiếng Việt (80+ từ)
   - Hỗ trợ tìm kiếm có dấu và không dấu
   - Encoding đồng nhất với response.content cho tất cả nguồn

3. Tìm kiếm thông minh:
   - 4 mức độ ưu tiên: phrase > có dấu > không dấu > fuzzy
   - BM25 ranking với field boosting
   - Highlight giải thích kết quả

Hướng phát triển:
- Tích hợp Vietnamese NLP (VnCoreNLP, Underthesea) cho word segmentation
- Semantic search với sentence embeddings (PhoBERT)
- Query suggestion và auto-complete

================================================================================
HẾT BÁO CÁO
================================================================================

